{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to display the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, display_html, display_markdown, HTML, Markdown as md\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, label_binarize, normalize\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
    "div.text_cell {font-family : DM Sans, sans-serif !important;}\n",
    "pre {font-family : DM Sans, sans-serif !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clasification Model Comparison Template** \n",
    "\n",
    "This Notebook compares the performance of different types of classification models on a dataset provided by the user. It is meant to assist in the model selection process, helping users discern the most suitible classification technique for their dataset. Throughout the template there are global variables that the user must configure which represent characteristics of the dataset. Unchanging characteristics like the name of the data file are represented with capitalized constant variables. Users should store data in the data directory.\n",
    "\n",
    "## **Dataset Overview**\n",
    "In this example, a customer segmentation dataset from Kaggle is used to demonstate the templates function. The goal is to classify customers into different segments based on their characteristics. The dataset contains customer information including demographic data and purchasing behavior. Our target variable is 'Segmentation', which categorizes customers into segments A, B, C, or D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load in a Dataset and Select Columns to Drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the data_file_name variable, input the name of the file containing your dataset (excluding /data. Example input: data_file.csv).\n",
    "\n",
    "In the columns_to_drop variable, input the name of columns to drop from you dataset (columns that dont contain any useful information for regression).\n",
    "\n",
    "In the dataset_na_value_representations variable, input all representations of missing values in your dataset that may not be automatically\n",
    "recognized as missing. Make sure no valid value in your dataset is included in this list.\n",
    "\"\"\"\n",
    "data_file_name: str = \"classification_segmentation_data.csv\" \n",
    "columns_to_drop: list[str] = [\"ID\"] \n",
    "dataset_na_value_representations = ['', 'NA', 'N/A', 'null', 'NULL', 'NaN', 'none', 'None', '-', '?']\n",
    "\n",
    "\n",
    "def display_dataframe(df: pd.DataFrame, font_size: int = 20) -> None:\n",
    "    \"\"\"\n",
    "    Displays the passed in DataFrame with the specified font size.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be displayed.\n",
    "        font_size (int): The font size at which the DataFrame should be displayed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df_html = df.to_html()\n",
    "    styled_html = f'<div style=\"font-size: {font_size}px;\">{df_html}</div>'\n",
    "    display_html(HTML(styled_html))\n",
    "    \n",
    "\n",
    "def display_text(text: str, font_size: int = 18, font_weight: str = 'normal') -> None:\n",
    "    \"\"\"\n",
    "    Displays the passed in text with the specified font size and font weight.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be displayed.\n",
    "        font_size (int): The font size at which the text should be displayed.\n",
    "        font_weight (str): The font weight (e.g., 'normal', 'bold', 'bolder', 'lighter', or numeric value from 100 to 900).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    styled_html = f'<div style=\"font-size: {font_size}px; font-weight: {font_weight};\">{text}</div>'\n",
    "    display_html(HTML(styled_html))\n",
    "    \n",
    "\n",
    "def load_data(file_name: str = data_file_name,\n",
    "              dropped_columns = columns_to_drop,\n",
    "              na_value_representations: list[str] = dataset_na_value_representations) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads in user's input file as a pandas DataFrame and converts various representations of missing values to NaN.\n",
    "    The file should be stored in the 'data' directory.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of file containing data for clustering\n",
    "        dropped_columns (list[str]): List of columns to drop from the dataframe\n",
    "        na_value_representations (list[str]): List of strings that represent missing values in the dataset\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Dataframe of variable values for all data entries\n",
    "    \"\"\"\n",
    "    # Automatically prepends 'data/' to the file name\n",
    "    file_name = \"data/\" + file_name\n",
    "    file_extension = file_name.split(\".\")[-1]\n",
    "\n",
    "    if file_extension == \"csv\":\n",
    "        df = pd.read_csv(file_name)\n",
    "    elif file_extension in [\"xls\", \"xlsx\"]:\n",
    "        df = pd.read_excel(file_name)\n",
    "    elif file_extension == \"json\":\n",
    "        df = pd.read_json(file_name)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format or misspelled file name. Please upload a CSV, Excel, or JSON file and ensure the file name is spelled correctly.\")\n",
    "    \n",
    "    # Replaces input representations of missing values with np.nan\n",
    "    df = df.replace(na_value_representations, np.nan)\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns = dropped_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "data_df: pd.DataFrame = load_data()\n",
    "initial_number_of_entries: int = len(data_df)\n",
    "\n",
    "variable_list: list[str] = list(data_df.columns)\n",
    "number_of_variables: int = len(variable_list)\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "# Converts all categorical variables to the Catagorical data type so they are recognized as such by ydata_profiling\n",
    "data_df[categorical_variables] = data_df[categorical_variables].astype(\"category\")\n",
    "\n",
    "# Initial ydata-profiling report of the dataset before missing values and outliers are handled\n",
    "#initial_dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (Before Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "#initial_dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Missing Values Information**\n",
    "The code in the following cell identifies numerical and categorical variables with missing values then provides users with useful infotmation for handling missing data. If missing values are present, it creates a DataFrame of rows with missing values and calculates summary statistics (number of entries with missing values, percentage of entries with missing values, and number of remaining entries in the dataset if all rows with missing values are dropped). These statistics are displayed along with up to the first 5 entries containing missing values. The displayed dataframe only includes columns that have one or more missing value. If there are no missing values in the dataset, it displays a message indicating so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically selects columns in the dataset with missing values, differentiating between numerical and categorical columns\n",
    "numerical_columns_with_missing_values: list[str] = data_df[numerical_variables].columns[data_df[numerical_variables].isnull().any()].tolist()\n",
    "categorical_columns_with_missing_values: list[str] = data_df[categorical_variables].columns[data_df[categorical_variables].isnull().any()].tolist()\n",
    "all_columns_with_missing_values: list[str] = numerical_columns_with_missing_values + categorical_columns_with_missing_values\n",
    "\n",
    "if len(all_columns_with_missing_values) != 0:\n",
    "    print()\n",
    "    # Dataframe containing all entries with missing values. The columns of this dataframe each have at least one missing value.\n",
    "    entries_with_missing_values_df: pd.DataFrame = data_df[all_columns_with_missing_values][data_df[all_columns_with_missing_values].isnull().any(axis = \"columns\")]\n",
    "    number_of_entries_with_missing_values: int = len(entries_with_missing_values_df)\n",
    "    percent_of_entries_with_missing_values: float = (number_of_entries_with_missing_values / initial_number_of_entries) * 100  \n",
    "    \n",
    "    display_text(f\"Total Number of Entries: {initial_number_of_entries}\")\n",
    "    display_text(f\"Total Number of Entreis with at Least One Missing Value: {number_of_entries_with_missing_values} ({percent_of_entries_with_missing_values:.2f}% of Entries)\")\n",
    "    display_text(f\"Number of Entries if all Rows with Missing Values are Dropped: {initial_number_of_entries - number_of_entries_with_missing_values}\")\n",
    "    print()\n",
    "    display_text(\"Up to First 5 Entries with Missing Values:\")\n",
    "    display_dataframe(entries_with_missing_values_df.head(), font_size = 16)\n",
    "else:\n",
    "    print()\n",
    "    display_text(\"No Missing Values in Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Missing Values**\n",
    "Use the code in the following cell to drop or impute missing values in your dataset. If you chose to impute missing values, edit the imputute_missing_values function to suite your dataset (such as choosing which columns to impute and specifying between imputing with the mean, median, or mode of the column). If called without further specification, the provided imputute_missing_values function is applied to all columns with missing values, imputing missing numerical values with the median of the column and missing categorical values with the mode of the column. By default, the cell uses the drop_rows_with_missing_values function to drop all entries with one or more missing value. The drop_rows_with_missing_values and impute_missing_values functions can be used in conjunction to drop rows with missing values in columns that cannot be imputed and then impute the remaining missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_missing_values(df: pd.DataFrame = data_df,\n",
    "                                  columns_to_check: list[str] = all_columns_with_missing_values) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes a copy of the input DataFrame and drops rows that have one or more missing values in any of the columns specified by \n",
    "    the columns_to_check parameter (does not mutate the input DataFrame). Also prints the number of entries dropped and the\n",
    "    resulting total number of entries.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing loded in data\n",
    "        columns_to_check (list[str]): List of columns to check for missing values\n",
    "    Returns:\n",
    "        dropna_df (pd.DataFrame): DataFrame with missing values dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    original_number_of_entries = len(df)\n",
    "    dropna_df = df.copy()\n",
    "    \n",
    "    dropna_df[columns_to_check] = dropna_df[columns_to_check].dropna()\n",
    "    new_number_of_entries = len(dropna_df)\n",
    "    number_of_entries_dropped = original_number_of_entries - new_number_of_entries\n",
    "    \n",
    "    display_text(f\"drop_rows_with_missing_values Results: {number_of_entries_dropped} Entries Dropped\") \n",
    "    display_text(f\"New Number of Entries: {new_number_of_entries}\")\n",
    "    \n",
    "    return dropna_df\n",
    "\n",
    "\n",
    "def impute_missing_values(df: pd.DataFrame = data_df,\n",
    "                          numerical_columns_to_impute: list[str] = numerical_columns_with_missing_values,\n",
    "                          categorical_columns_to_impute: list[str] = categorical_columns_with_missing_values) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values in the DataFrame with either the median value (for numerical variables) or the most frequent value\n",
    "    (for categorical variables).\n",
    "    \n",
    "    Args:\n",
    "        numerical_columns_to_impute (list[str]): List of the names of numerical columns with missing values to impute\n",
    "        categorical_columns_to_impute (list[str]): List of the names of categorical columns with missing values to impute\n",
    "    Returns:\n",
    "        impute_df (pd.DataFrame): DataFrame with missing values imputed\n",
    "    \"\"\"\n",
    "    impute_df = df.copy()\n",
    "    \n",
    "    # Here is where to configure the imputation strategy if need be\n",
    "    numerical_imputer = SimpleImputer(strategy = \"median\")\n",
    "    categorical_imputer = SimpleImputer(strategy = \"most_frequent\")\n",
    "    \n",
    "    impute_df[numerical_columns_to_impute] = numerical_imputer.fit_transform(impute_df[numerical_columns_to_impute])\n",
    "    impute_df[categorical_columns_to_impute] = categorical_imputer.fit_transform(impute_df[categorical_columns_to_impute])\n",
    "    \n",
    "    display_text(\"Missing Values Successfully Imputed\")\n",
    "    \n",
    "    return impute_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Specify how you would like to handle missing values in the dataset, setting a variable to hold your new dataset.\n",
    "\"\"\"\n",
    "data_df = drop_rows_with_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Outliers/Eronious Entries**\n",
    "Use the visualize_outliers function to analyze numerical columns of your dataset for outliers/eronious inputs and optionally remove them. When removing entries with the visualize_outliers function, it is easiest to work with one column at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayed to compare dataset before and after handling outliers/eronious entries\n",
    "display_text(\"Numerical Variables Information\", font_size = 20)\n",
    "display_dataframe(data_df.describe(), 16) \n",
    "\n",
    "def visualize_outliers(df: pd.DataFrame = data_df,\n",
    "                       numerical_columns_to_check: Union[list[str], str] = numerical_variables,\n",
    "                       iqr_multiplier: float = 1.5,\n",
    "                       remove: bool = False,\n",
    "                       remove_option: str = 'both',\n",
    "                       display: bool = True,) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a boxplot for each column of the input Dataframe in the numerical_columns_to_check parameter to help users visualize potential\n",
    "    outliers in their dataset. Below this boxplot, the function prints the number of high and low outliers (determined by the IQR method) in the\n",
    "    current column. The upper and lower bounds for outliers are denoted by red dotted lines. Points below the low bound red dotted line\n",
    "    or above the high bound red dotted line are consideered outliers. Users can choose whether to drop outlier entries through the remove\n",
    "    boolean parameter. You can change which points are considered outliers by changing the iqr_multiplier parameter.\n",
    "    \n",
    "    The lower and upper whiskers of the boxplot denote the 5th and 95th percentile of the current column's values respectively.\n",
    "\n",
    "    This function can be used iteratively to handle outliers in different columns with varying sensitivity levels. It allows for\n",
    "    selective removal of entries below/above the red dotted lines. The function can be run without displaying visualizations for efficiency.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "        numerical_columns_to_check (Union[list[str], str], default = numerical_variables): List of the names of columns to check for outliers. The\n",
    "            default argument is a list of all numerical columns in the input DataFrame.\n",
    "        iqr_multiplier (float, default = 1.5): Multiplier for the IQR to define the outlier threshold. Higher values are more lenient, increasing\n",
    "            the range of the upper and lower red dotted lines. Lower values are more strict, decreasing the range of the red dotted lines.\n",
    "        remove (bool, default = False): If True, removes identified outliers from the DataFrame.\n",
    "        remove_option (str, default = 'both'): Specifies which outliers to remove: 'both' removes all identified outliers, 'upper' only removes\n",
    "            outliers greater than the upper bound (values past the upper red dotted line), and 'lower' only removes outliers less than the\n",
    "            lower bound (values behind the lower red dotted line). This parameter has no effect if remove = False.\n",
    "        display (bool, default = True): If True, displays boxplots for each variable. If false, only outlier statistics are printed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame if remove = False, otherwise a new DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    # If a single column is passed in as a string, convert it to a list so the following for loop still works properly\n",
    "    if type(numerical_columns_to_check) == str:\n",
    "        numerical_columns_to_check = [numerical_columns_to_check]\n",
    "        \n",
    "    for col in numerical_columns_to_check:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (iqr * iqr_multiplier)\n",
    "        upper_bound = q3 + (iqr * iqr_multiplier)\n",
    "        \n",
    "        # Only create plot if display is True\n",
    "        if display:\n",
    "            plt.figure(figsize = (10, 6))\n",
    "            ax = sns.boxplot(x = df[col], whis = [5, 95])\n",
    "            plt.title(f'Boxplot of {col}')\n",
    "            \n",
    "            # Add vertical red dotted lines for lower and upper bounds if within the plot's x-axis limits\n",
    "            x_min, x_max = ax.get_xlim()\n",
    "            if x_min <= lower_bound <= x_max:\n",
    "                plt.axvline(lower_bound, color='red', linestyle='dotted', linewidth=1)\n",
    "            if x_min <= upper_bound <= x_max:\n",
    "                plt.axvline(upper_bound, color='red', linestyle='dotted', linewidth=1)\n",
    "            \n",
    "            # Create legend\n",
    "            legend_lines = [Line2D([0], [0], color='red', linestyle='dotted', linewidth=1)]\n",
    "            legend_labels = ['Lower/Upper Bound']\n",
    "            plt.legend(legend_lines, legend_labels, loc='upper right')\n",
    "            plt.show()\n",
    "        \n",
    "        lower_outlier_count = df[col][df[col] < lower_bound].count()\n",
    "        upper_outlier_count = df[col][df[col] > upper_bound].count()\n",
    "        \n",
    "        display_text(f\"{col}:\", font_size = 18, font_weight = 'bold')\n",
    "        display_text(f\"- Lower Bound for Outliers: {lower_bound}\", font_size = 16)\n",
    "        display_text(f\"- Upper Bound for Outliers: {upper_bound}\", font_size = 16)\n",
    "        display_text(f\"- Number of Outliers Below Lower Bound: {lower_outlier_count}\", font_size = 16)\n",
    "        display_text(f\"- Number of Outliers Above Upper Bound: {upper_outlier_count}\", font_size = 16)\n",
    "        print()\n",
    "        \n",
    "    # Removes outliers from the DataFrame if remove = True\n",
    "    if remove:\n",
    "        # Calculate indices of outliers\n",
    "        lower_outlier_indices = df.index[df[col] < lower_bound].tolist()\n",
    "        upper_outlier_indices = df.index[df[col] > upper_bound].tolist()\n",
    "        outlier_indices_to_be_removed = set()\n",
    "        \n",
    "        # Add outlier indices that will be removed to outlier_indices_to_be_removed based on the remove_option parameter\n",
    "        if remove_option == \"both\":\n",
    "            outlier_indices_to_be_removed.update(lower_outlier_indices)\n",
    "            outlier_indices_to_be_removed.update(upper_outlier_indices)\n",
    "        elif remove_option == \"lower\":\n",
    "            outlier_indices_to_be_removed.update(lower_outlier_indices)\n",
    "        elif remove_option == \"upper\":\n",
    "            outlier_indices_to_be_removed.update(upper_outlier_indices)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument passed into remove_option parameter. Please use 'both', 'lower', or 'upper'.\")\n",
    "            \n",
    "        removed_outliers_df = df.drop(index = outlier_indices_to_be_removed)\n",
    "        display_text(f\"Total Number of Outlier Entries Removed in {col}: {len(outlier_indices_to_be_removed)}\", font_size = 18)\n",
    "        print()\n",
    "        return removed_outliers_df\n",
    "    \n",
    "    # Simply return the original DataFrame if remove = False\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use the visualize_outliers function to identify and optionally remove outliers in the numerical columns of your dataset. To effectively\n",
    "handle outliers, you may need to run this function multiple times with different sensitivity levels (iqr_multiplier) for different\n",
    "columns. This means you may need to run the function multiple times with different columns_to_check parameters, handling each column\n",
    "individually.\n",
    "\"\"\"  \n",
    "data_df = visualize_outliers(data_df, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Preprocessing Information**\n",
    "Use the information provided by the comparison profile report to analyze the result of your data cleaning and to help determine your preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (After Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "post_cleaning_comparison_report = dataset_report.compare(initial_dataset_report)\n",
    "post_cleaning_comparison_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Steps**\n",
    "Define your preprocessing steps in the code cell below. The provided steps address three common preprocessing transformations: scaling numerical variables, one-hot encoding nominal categorical variables, and ordianal encoding ordinal categorical variables. Customize these steps to fit the needs of your dataset. Preprocessing for your target variable and feature variables must be handled by seperate transformer variables.\n",
    "\n",
    "The feature variable preprocessing steps will be passed into the Scikit Learn make_pipeline function when models are loaded in.\n",
    "\n",
    "The target variable preprocessing steps will be applied directly to the y_train and y_test sets after the dataset is split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN_NAME: str = \"Segmentation\"\n",
    "\n",
    "\n",
    "numerical_features_to_scale: list[str] = list(set(numerical_variables) - set([TARGET_COLUMN_NAME]))\n",
    "nominal_categorical_features_to_encode: list[str] = list(set(categorical_variables) - set([TARGET_COLUMN_NAME]))\n",
    "\n",
    "# Order ordinal variable categories from lowest to highest \n",
    "ordianl_categories_ordered_dict: dict[str, list[str]] = {\n",
    "    \"Spending_Score\": ['Low', 'Average', 'High']\n",
    "}\n",
    "ordianl_features_categories_to_encode: list[str] = list(ordianl_categories_ordered_dict.keys())\n",
    "ordianl_features_categories_orders_lists: list[list[str]] = list(ordianl_categories_ordered_dict.values())\n",
    "\n",
    "# Indicates that the first column of one-hot encoded variables should be dropped to avoid multicollinearity\n",
    "onehot_drop_column = \"first\"\n",
    "\n",
    "\n",
    "# Pass in a list of tuples containing a name for the transformer (decide a name, allows transformer parameters to be searched in grid search),\n",
    "# the transformer object, and the columns to apply the transformer to\n",
    "general_feature_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('numerical_scaler', StandardScaler(), numerical_features_to_scale),\n",
    "        ('nominal_encoder', OneHotEncoder(drop = onehot_drop_column), nominal_categorical_features_to_encode),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories = ordianl_features_categories_orders_lists), ordianl_features_categories_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setup target preprocessor\n",
    "nominal_target_preprocessor = LabelEncoder()\n",
    "\n",
    "display_markdown(md(f\"### **Scaled Numerical Features:** {numerical_features_to_scale}\"))\n",
    "display_markdown(md(f\"### **Encoded Nominal Categorical Features:** {nominal_categorical_features_to_encode}\"))\n",
    "display_markdown(md(f\"### **Encoded Ordinal Categorical Features (confirm that category orders were assigned to the correct ordinal categorical feature):**\"))\n",
    "for i in range(len(ordianl_features_categories_to_encode)):\n",
    "    display_markdown(md(f\"* #### **{ordianl_features_categories_to_encode[i]}:** {ordianl_features_categories_orders_lists[i]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the target variable column in the dataset\n",
    "\"\"\"\n",
    "target_column_name: str = \"GradeClass\"\n",
    "\n",
    "\"\"\"\n",
    "From here, you must create preprocessing steps specific  to your dataset. The following is a general preprocessing setup that can be\n",
    "easily adapted to work on most datasets:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Input a list of the numerical features you would like to scale (defaultes to all numerical features in the dataset) and a list of\n",
    "the nominal categorical features you would like to one-hot encode.\n",
    "\"\"\"\n",
    "numerical_features_to_scale: list[str] = [\"Age\", \"StudyTimeWeekly\", \"Absences\", ]\n",
    "nominal_categorical_features_to_encode: list[str] = [\"Ethnicity\"]\n",
    "\n",
    "\"\"\"\n",
    "In the ordianl_categories_ordered_dict variable, input the order of ordinal categorical variable categories into a dictionary. The keys\n",
    "of this dictionary should be names of ordinal categorical variables and the values should be lists of the categories in order from smallest\n",
    "to largest.\n",
    "\"\"\"\n",
    "ordianl_categories_ordered_dict: dict[str, list[str]] = {\n",
    "    \"ParentalEducation\": [0, 1, 2, 3, 4],\n",
    "    \"ParentalSupport\": [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# Extracts the keys and values of the ordianl_categories_ordered_dict into separate lists\n",
    "ordianl_feature_categories_to_encode: list[str] = list(ordianl_categories_ordered_dict.keys())\n",
    "ordianl_feature_categories_orders_lists: list[list[str]] = list(ordianl_categories_ordered_dict.values())\n",
    "\n",
    "# Indicates that the first column of one-hot encoded variables should be dropped to avoid multicollinearity\n",
    "onehot_drop_column: str = \"first\"\n",
    "\n",
    "\n",
    "# The argumnet for the transformers parameter of ColumnTransformer must be a a list of touples with three entries. Each of these touples\n",
    "# represents a preprocessing step. The first entry of each touple is a name for the step. The second entry is the transformer object, and\n",
    "# the final entry is a list of the columns the step should be applied to.\n",
    "general_feature_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('numerical_scaler', StandardScaler(), numerical_features_to_scale),\n",
    "        ('nominal_encoder', OneHotEncoder(drop = onehot_drop_column), nominal_categorical_features_to_encode),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories = ordianl_feature_categories_orders_lists), ordianl_feature_categories_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Specify the preprocessor used on the target variable column.\n",
    "\"\"\"\n",
    "numerical_target_preprocessor = StandardScaler()\n",
    "\n",
    "display_text(f\"Scaled Numerical Variables: {numerical_features_to_scale}\")\n",
    "display_text(f\"Encoded Nominal Categorical Variables: {nominal_categorical_features_to_encode}\")\n",
    "display_text(f\"Encoded Ordinal Categorical Variables (confirm that category orders were assigned to the correct ordinal categorical variable):\")\n",
    "if len(ordianl_feature_categories_to_encode) != 0:\n",
    "    for i in range(len(ordianl_feature_categories_to_encode)):\n",
    "        display_markdown(md(f\"* {ordianl_feature_categories_to_encode[i]}: {ordianl_feature_categories_orders_lists[i]}\"))\n",
    "else:\n",
    "    display_text(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load In Models and Set Hyperparameters**\n",
    "Define the functions for loading each model. Use the make_pipeline function to combine the preprocessing steps with the model and set the hyperparameters for each model to be optimized by GridSearchCV.\n",
    "\n",
    "The make_pipeline function changes the way you have to input values in each models param_grid. In order for GridSearchCV to work with the make_pipline function, model hyeperparameter keys must be prefixed with the model name followed by two underscores (ex: the hyperparameter for the Lasso Regression model is \"lasso__alpha\" instead of just \"alpha\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the models you would like to compare. Assign a variable for each model to a dictionary. The key of each dictionary should be a string containing\n",
    "the name of the model. The value should be another dictionary with two string keys: \"model\" and \"param_grid\". The value of the \"model\" key should be a\n",
    "pipeline (using the scikit-learn make_pipline function) that that combines your preprocessor with the model. The value of the \"param_grid\" key should be\n",
    "a dictionary that defines the hyperparameters to be optimized for the model. In the \"param_grid\" dictionary, the keys should be model hyperparmeters and\n",
    "the values should be list-like objects containing values to be tested for each hyperparameter by GridSearchCV. In order for GridSearchCV to recognize the\n",
    "hyperparameters of a model that is part of a pipeline, the hyperparameter keys in the \"param_grid\" dictionary should be prefixed with the name of the\n",
    "model followed by two underscores. For example, if you want to optimize the n_neighbors hyperparameter of a KNeighborsClassifier model, the key in the\n",
    "\"param_grid\" dictionary for the alpha hyperparameter should be \"kneighborsclassifier__n_neighbors\". The prefixed model names are the same as the\n",
    "scikit-learn model names in all lower case letters.\n",
    "\"\"\"\n",
    "logistic_model_data = {\n",
    "    'Logistic Regression': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, LogisticRegression(max_iter=1000)),\n",
    "        'param_grid': {\n",
    "            'logisticregression__C': [0.1, 1, 10],\n",
    "            'logisticregression__solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "kneighbors_model_data = {\n",
    "    'KNN': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, KNeighborsClassifier()),\n",
    "        'param_grid': {\n",
    "            'kneighborsclassifier__n_neighbors': [3, 5, 7],\n",
    "            'kneighborsclassifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "decision_tree_model_data = {\n",
    "    'Decision Tree': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, DecisionTreeClassifier(random_state=42)),\n",
    "        'param_grid': {\n",
    "            'decisiontreeclassifier__max_depth': [None, 5, 10],\n",
    "            'decisiontreeclassifier__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "random_forest_model_data = {\n",
    "    'Random Forest': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, RandomForestClassifier(random_state=42)),\n",
    "        'param_grid': {\n",
    "            'randomforestclassifier__n_estimators': [100, 200],\n",
    "            'randomforestclassifier__max_depth': [None, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "svc_model_data = {\n",
    "        'SVC': {\n",
    "            'model': make_pipeline(general_feature_preprocessor, SVC(probability=True, random_state=42)),\n",
    "            'param_grid': {\n",
    "                'svc__C': [0.1, 1, 10],\n",
    "                'svc__kernel': ['linear', 'rbf']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "gaussian_nb_model_data = {\n",
    "    'Naive Bayes': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, GaussianNB()),\n",
    "        'param_grid': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    **logistic_model_data,\n",
    "    **kneighbors_model_data,\n",
    "    **decision_tree_model_data,\n",
    "    **random_forest_model_data,\n",
    "    **svc_model_data,\n",
    "    **gaussian_nb_model_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training and Evaluation**\n",
    "Split the data into training and testing sets using the scikit-learn train_test_split function and transform the target variable using your target preprocessing steps. After this, the train_and_evaluate_models function will have access to all of it's necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DECIMAL_PLACES: int = 7 # Determines the number of decimal places to display for model evaluation metrics\n",
    "\n",
    "\n",
    "\n",
    "X = data_df.drop(columns = [TARGET_COLUMN_NAME])\n",
    "y = data_df[TARGET_COLUMN_NAME]\n",
    "\n",
    "FEATURE_LIST = list(X.columns)\n",
    "NUM_FEATURES: int = len(FEATURE_LIST)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "y_train = nominal_target_preprocessor.fit_transform(y_train)\n",
    "y_test = nominal_target_preprocessor.fit_transform(y_test)\n",
    "\n",
    "def train_and_evaluate_models(models_dict: dict[dict[str, dict]] = models,\n",
    "                              X_train: pd.DataFrame = X_train,\n",
    "                              X_test: pd.DataFrame = X_test,\n",
    "                              y_train: np.ndarray = y_train,\n",
    "                              y_test: np.ndarray = y_test) -> dict:\n",
    "    \"\"\"\n",
    "    Optimizes the hyperparameters of, trains, and evaluates the performance of all passed in models on the training and testing data.\n",
    "    Also saves information from the training process such as best model, predictions, and training/testing time.\n",
    "    \n",
    "    Args:\n",
    "        models_dict (dict[dict[str, dict]]): Dictionary containing model names as keys and dictionaries containing the model object and hyperparameter grid as values\n",
    "        X_train (pd.DataFrame): DataFrame containing feature variable values for training the models\n",
    "        X_test (pd.DataFrame): DataFrame containing feature variable values for testing the models\n",
    "        y_train (np.ndarray): 1D np.ndarray containing target variable values for training the models\n",
    "        y_test (np.ndarray): 1D np.ndarray containing target variable values for testing the models\n",
    "    Returns:\n",
    "        model_results (dict): Dictionary that has model names as its keys. The value for these keys are dictionaries containing the trained model object,\n",
    "        model predictions on the testing data, and other data from the training process such as accuracy, precision, recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for model_name, model_data in models_dict.items():\n",
    "        display_markdown(md(f\"### **Training and evaluating: {model_name}**\"))\n",
    "        \n",
    "        model = model_data['model']\n",
    "        param_grid = model_data['param_grid']\n",
    "        \n",
    "        stratifiedkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=stratifiedkf, scoring='accuracy')\n",
    "        \n",
    "        tune_train_start_time = time.time()\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        tune_train_end_time = time.time()\n",
    "        tune_train_time = tune_train_end_time - tune_train_start_time\n",
    "        display_markdown(md(f\"* #### **Hyperparameter Tuning and Model Training Time:** {tune_train_time:.2f} seconds\"))\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        y_train_predictions = best_model.predict(X_train)\n",
    "        y_test_predictions = best_model.predict(X_test)\n",
    "        y_test_prediction_probabilities = best_model.predict_proba(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_test_predictions)\n",
    "        precision = precision_score(y_test, y_test_predictions, average='weighted')\n",
    "        recall = recall_score(y_test, y_test_predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, y_test_predictions, average='weighted')\n",
    "        \n",
    "        display_markdown(md(f\"* #### **Test Accuracy:** {accuracy:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "        display_markdown(md(f\"* #### **Test Precision:** {precision:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "        display_markdown(md(f\"* #### **Test Recall:** {recall:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "        display_markdown(md(f\"* #### **Test F1 Score:** {f1:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "        \n",
    "        model_results[model_name] = {\n",
    "            'best_model': best_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            \"cv_results\": grid_search.cv_results_,\n",
    "            \"tune_train_time\": tune_train_time,\n",
    "            \"y_train_predictions\": y_train_predictions,\n",
    "            \"y_test_predictions\": y_test_predictions,\n",
    "            \"y_test_prediction_probabilities\": y_test_prediction_probabilities,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "model_results = train_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_model_performance(model_results: dict[str, dict[str, object]]):\n",
    "    \"\"\"\n",
    "    Creates a grouped bar chart to compare performance metrics across different classification models.\n",
    "\n",
    "    Args:\n",
    "    model_results (Dict[str, Dict[str, Any]]): Dictionary containing model names as keys and their results as values.\n",
    "                                               Each model's results should include 'f1', 'accuracy', 'precision', and 'recall'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract model names and metrics\n",
    "    models = list(model_results.keys())\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    f1_scores = [model_results[model]['f1'] for model in models]\n",
    "    accuracy_scores = [model_results[model]['accuracy'] for model in models]\n",
    "    precision_scores = [model_results[model]['precision'] for model in models]\n",
    "    recall_scores = [model_results[model]['recall'] for model in models]\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    fig.suptitle('Comparative Model Performance (Higher is Better for All)', fontsize=24)\n",
    "    \n",
    "    # Add separation between the main title and subplots\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Function to plot metric and set y-axis limits\n",
    "    def plot_metric(ax, scores, title, color):\n",
    "        x = np.arange(len(models))  # the label locations\n",
    "        ax.bar(x, scores, color=color)\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'{title} Scores by Model', fontweight='bold')\n",
    "        \n",
    "        # Set tick locations and labels\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        \n",
    "        # Set y-axis limits relative to the scores\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        range_score = max_score - min_score\n",
    "        ax.set_ylim(max(0, min_score - 0.1 * range_score), min(1, max_score + 0.1 * range_score))\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for i, v in enumerate(scores):\n",
    "            ax.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot F1 scores\n",
    "    plot_metric(axs[0, 0], f1_scores, 'F1 Score', 'skyblue')\n",
    "    \n",
    "    # Plot Accuracy scores\n",
    "    plot_metric(axs[0, 1], accuracy_scores, 'Accuracy', 'lightgreen')\n",
    "    \n",
    "    # Plot Precision scores\n",
    "    plot_metric(axs[1, 0], precision_scores, 'Precision', 'salmon')\n",
    "    \n",
    "    # Plot Recall scores\n",
    "    plot_metric(axs[1, 1], recall_scores, 'Recall', 'gold')\n",
    "\n",
    "    # Add grid to all subplots\n",
    "    for ax in axs.flat:\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.975])  # Add space at the top for the main title\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_comparative_model_performance(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results: dict = model_results):\n",
    "    \"\"\"\n",
    "    Displays the performance metrics of all models trained and evaluated in the train_and_evaluate_models function in a DataFrame\n",
    "    and returns the name of the best model based on accuracy score. The model with the highest accuracy score is considered the best model.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing model names as keys and dictionaries containing the trained model object,\n",
    "                        predictions on the testing data, accuracy, precision, recall, F1 score, and other model performance data as values\n",
    "    Returns:\n",
    "        best_performing_model (str): Name of the best model based on accuracy score\n",
    "    \"\"\"\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': results.keys(),\n",
    "        'F1 Score': [result['f1'] for result in results.values()],\n",
    "        'Accuracy': [result['accuracy'] for result in results.values()],\n",
    "        'Precision': [result['precision'] for result in results.values()],\n",
    "        'Recall': [result['recall'] for result in results.values()]\n",
    "    })\n",
    "\n",
    "    display(md(\"## Model Performance Summary\"))\n",
    "    display_dataframe(summary_df)\n",
    "    print()\n",
    "\n",
    "    best_performing_model = summary_df.loc[summary_df['F1 Score'].idxmax(), 'Model']\n",
    "    display(md(f\"## **Best Performing Model: {best_performing_model}** (based on F1 score, the higher the better)\"))\n",
    "    display(md(f\"* ### F1 Score: {results[best_performing_model]['f1']:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display(md(f\"* ### Accuracy Score: {results[best_performing_model]['accuracy']:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display(md(f\"* ### Precision Score: {results[best_performing_model]['precision']:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display(md(f\"* ### Recall Score: {results[best_performing_model]['recall']:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "\n",
    "    return best_performing_model\n",
    "\n",
    "best_model = summarize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Model Performance Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = nominal_target_preprocessor.classes_\n",
    "\n",
    "def plot_confusion_matrices_comparison(model_results: dict[str, dict], y_test: np.ndarray, target_classes: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plots confusion matrices for multiple classification models in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "    model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "                                     and each value is a dictionary containing model predictions\n",
    "    y_test (np.ndarray): Actual target values\n",
    "    target_classes (np.ndarray): Array of target class names\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 10*num_rows))\n",
    "    \n",
    "    fig.suptitle('Comparison of Confusion Matrices', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_pred = results['y_test_predictions']\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_classes)\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", values_format='d')\n",
    "        \n",
    "        ax.set_title(f'{model_name} Confusion Matrix', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Calculate and display accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        ax.text(0.5, -0.1, f'Accuracy: {accuracy:.3f}', \n",
    "                horizontalalignment='center', verticalalignment='center', \n",
    "                transform=ax.transAxes, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.3, wspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_roc_curves_comparison(model_results: dict[str, dict], y_test: np.ndarray, target_classes: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plots ROC curves for multiple classification models in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "    model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "                                     and each value is a dictionary containing model predictions\n",
    "    y_test (np.ndarray): Actual target values\n",
    "    target_classes (np.ndarray): Array of target class names\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 10*num_rows))\n",
    "    \n",
    "    fig.suptitle('Comparison of ROC Curves', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    n_classes = len(target_classes)\n",
    "    y_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_prediction_probabilities = results['y_test_prediction_probabilities']\n",
    "        \n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            \n",
    "            ax.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,\n",
    "                    label=f'ROC curve of class {target_classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f'{model_name} ROC Curve', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.legend(loc=\"lower right\", fontsize=10)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout(rec)\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.3, wspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curves_comparison(model_results: dict[str, dict], y_test: np.ndarray, target_classes: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plots Precision-Recall curves for multiple classification models in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "    model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "                                     and each value is a dictionary containing model predictions\n",
    "    y_test (np.ndarray): Actual target values\n",
    "    target_classes (np.ndarray): Array of target class names\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 10*num_rows))\n",
    "    \n",
    "    fig.suptitle('Comparison of Precision-Recall Curves', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    n_classes = len(target_classes)\n",
    "    y_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_prediction_probabilities = results['y_test_prediction_probabilities']\n",
    "        \n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        avg_precision = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "            avg_precision[i] = average_precision_score(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "            \n",
    "            ax.plot(recall[i], precision[i], color=colors[i % len(colors)], lw=2,\n",
    "                    label=f'PR curve of class {target_classes[i]} (AUC-PR = {avg_precision[i]:.2f})')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title(f'{model_name} Precision-Recall Curve', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.legend(loc=\"lower left\", fontsize=10)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.3, wspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_learning_curves_comparison(model_results: dict[str, dict], X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plots learning curves for multiple classification models in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "    model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "                                     and each value is a dictionary containing the trained model\n",
    "    X (np.ndarray): Feature matrix\n",
    "    y (np.ndarray): Target vector\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 10*num_rows))\n",
    "    \n",
    "    fig.suptitle('Comparison of Learning Curves', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        estimator = results['best_model']\n",
    "        \n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=5, n_jobs=-1, \n",
    "            train_sizes=np.linspace(.1, 1.0, 5))\n",
    "        \n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "        ax.set_title(f\"Learning Curve: {model_name}\", fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel(\"Training examples\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.fill_between(train_sizes, (train_scores_mean - train_scores_std),\n",
    "                        (train_scores_mean + train_scores_std), alpha=0.1, color=\"r\")\n",
    "        ax.fill_between(train_sizes, (test_scores_mean - test_scores_std),\n",
    "                        (test_scores_mean + test_scores_std), alpha=0.1, color=\"g\")\n",
    "        ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Set y-axis limits\n",
    "        ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.4, wspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_confusion_matrices_comparison(model_results, y_test, target_classes)\n",
    "plot_roc_curves_comparison(model_results, y_test, target_classes)\n",
    "plot_precision_recall_curves_comparison(model_results, y_test, target_classes)\n",
    "plot_learning_curves_comparison(model_results, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Individual Model Performance Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_model_evaluation_results(model_name: str, results: dict = model_results) -> None:\n",
    "    \"\"\"\n",
    "    Displays the evaluation results for a given model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        results (dict): Dictionary containing the evalutation results of each model\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    display_markdown(md(f\"### **Model: {model_name}**\"))\n",
    "    display_markdown(md(f\"* #### **F1 Score:** {results[model_name][\"f1\"]:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display_markdown(md(f\"* #### **Accuracy:** {results[model_name][\"accuracy\"]:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display_markdown(md(f\"* #### **Precision:** {results[model_name][\"precision\"]:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "    display_markdown(md(f\"* #### **Recall:** {results[model_name][\"recall\"]:.{NUM_DECIMAL_PLACES}f}\"))\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model_name, y_true, y_predictions, classes = target_classes):\n",
    "    cm = confusion_matrix(y_true, y_predictions)\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = classes)\n",
    "    disp.plot(cmap = \"Blues\")\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_roc_curve(model_name, y_true, y_prediction_probabilities, classes = target_classes):\n",
    "    n_classes = len(classes)\n",
    "    y_bin = label_binarize(y_true, classes = range(n_classes))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    plt.figure(figsize = (10, 8))\n",
    "    for i, color in zip(range(n_classes), ['blue', 'red', 'green', 'orange']):\n",
    "        plt.plot(fpr[i], tpr[i], color = color, lw = 2,\n",
    "                 label = f'ROC curve of class {classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw = 2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_multiclass_precision_recall_curve(model_name, y_true, y_prediction_probabilities, classes = target_classes):\n",
    "    n_classes = len(classes)\n",
    "    y_bin = label_binarize(y_true, classes = range(n_classes))\n",
    "    \n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_bin[:, i], y_prediction_probabilities[:, i])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, color in zip(range(n_classes), ['blue', 'red', 'green', 'orange']):\n",
    "        plt.plot(recall[i], precision[i], color = color, lw = 2,\n",
    "                 label = f'Precision-Recall curve of class {classes[i]} (AUC = {avg_precision[i]:.2f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} Precision-Recall Curve')\n",
    "    plt.legend(loc = \"lower left\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "feature_columns = list(X_train.columns)\n",
    "def get_featuere_names(X_column_names = feature_columns):\n",
    "    feature_names = [column_name.replace(\"_\", \" \") for column_name in X_column_names]\n",
    "    return feature_names\n",
    "\n",
    "\n",
    "def plot_feature_importance(model_name, model, feature_names = FEATURE_LIST):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"{model_name} Feature Importances\")\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation = 90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_learning_curve(estimator, X, y, model_name):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=-1, \n",
    "        train_sizes = np.linspace(.1, 1.0, 5))\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis = 1)\n",
    "    train_scores_std = np.std(train_scores, axis = 1)\n",
    "    test_scores_mean = np.mean(test_scores, axis = 1)\n",
    "    test_scores_std = np.std(test_scores, axis = 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Learning Curve: {model_name}\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.fill_between(train_sizes, (train_scores_mean - train_scores_std),\n",
    "                     (train_scores_mean + train_scores_std), alpha = 0.1, color = \"r\")\n",
    "    plt.fill_between(train_sizes, (test_scores_mean - test_scores_std),\n",
    "                     (test_scores_mean + test_scores_std), alpha = 0.1, color = \"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\", label = \"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\", label = \"Cross-validation score\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_name = \"Logistic Regression\"\n",
    "\n",
    "logistic_best_model = model_results[logistic_model_name][\"best_model\"]\n",
    "\n",
    "logistic_y_test_predictions = model_results[logistic_model_name][\"y_test_predictions\"]\n",
    "logistic_y_test_prediction_probabilities = model_results[logistic_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "logistic_f1_score = model_results[logistic_model_name][\"f1\"]\n",
    "logistic_accuracy = model_results[logistic_model_name][\"accuracy\"]\n",
    "logistic_precision = model_results[logistic_model_name][\"precision\"]\n",
    "logistic_recall = model_results[logistic_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(logistic_model_name)\n",
    "plot_confusion_matrix(logistic_model_name, y_test, logistic_y_test_predictions)\n",
    "plot_roc_curve(logistic_model_name, y_test, logistic_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(logistic_model_name, y_test, logistic_y_test_prediction_probabilities)\n",
    "plot_learning_curve(logistic_best_model, X_train, y_train, logistic_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_name = \"KNN\"\n",
    "\n",
    "knn_best_model = model_results[knn_model_name][\"best_model\"]\n",
    "\n",
    "knn_y_test_predictions = model_results[knn_model_name][\"y_test_predictions\"]\n",
    "knn_y_test_prediction_probabilities = model_results[knn_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "knn_f1_score = model_results[knn_model_name][\"f1\"]\n",
    "knn_accuracy = model_results[knn_model_name][\"accuracy\"]\n",
    "knn_precision = model_results[knn_model_name][\"precision\"]\n",
    "knn_recall = model_results[knn_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(knn_model_name)\n",
    "plot_confusion_matrix(knn_model_name, y_test, knn_y_test_predictions)\n",
    "plot_roc_curve(knn_model_name, y_test, knn_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(knn_model_name, y_test, knn_y_test_prediction_probabilities)\n",
    "plot_learning_curve(knn_best_model, X_train, y_train, knn_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model_name = \"Decision Tree\"\n",
    "\n",
    "decision_tree_best_model = model_results[decision_tree_model_name][\"best_model\"]\n",
    "\n",
    "decision_tree_y_test_predictions = model_results[decision_tree_model_name][\"y_test_predictions\"]\n",
    "decision_tree_y_test_prediction_probabilities = model_results[decision_tree_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "decision_tree_f1_score = model_results[decision_tree_model_name][\"f1\"]\n",
    "decision_tree_accuracy = model_results[decision_tree_model_name][\"accuracy\"]\n",
    "decision_tree_precision = model_results[decision_tree_model_name][\"precision\"]\n",
    "decision_tree_recall = model_results[decision_tree_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(decision_tree_model_name)\n",
    "plot_confusion_matrix(decision_tree_model_name, y_test, decision_tree_y_test_predictions)\n",
    "plot_roc_curve(decision_tree_model_name, y_test, decision_tree_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(decision_tree_model_name, y_test, decision_tree_y_test_prediction_probabilities)\n",
    "plot_learning_curve(decision_tree_best_model, X_train, y_train, decision_tree_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model_name = \"Random Forest\"\n",
    "\n",
    "random_forest_best_model = model_results[random_forest_model_name][\"best_model\"]\n",
    "\n",
    "random_forest_y_test_predictions = model_results[random_forest_model_name][\"y_test_predictions\"]\n",
    "random_forest_y_test_prediction_probabilities = model_results[random_forest_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "random_forest_f1_score = model_results[random_forest_model_name][\"f1\"]\n",
    "random_forest_accuracy = model_results[random_forest_model_name][\"accuracy\"]\n",
    "random_forest_precision = model_results[random_forest_model_name][\"precision\"]\n",
    "random_forest_recall = model_results[random_forest_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(random_forest_model_name)\n",
    "plot_confusion_matrix(random_forest_model_name, y_test, random_forest_y_test_predictions)\n",
    "plot_roc_curve(random_forest_model_name, y_test, random_forest_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(random_forest_model_name, y_test, random_forest_y_test_prediction_probabilities)\n",
    "plot_learning_curve(random_forest_best_model, X_train, y_train, random_forest_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_name = \"SVC\"\n",
    "\n",
    "svc_best_model = model_results[svc_model_name][\"best_model\"]\n",
    "\n",
    "svc_y_test_predictions = model_results[svc_model_name][\"y_test_predictions\"]\n",
    "svc_y_test_prediction_probabilities = model_results[svc_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "svc_f1_score = model_results[svc_model_name][\"f1\"]\n",
    "svc_accuracy = model_results[svc_model_name][\"accuracy\"]\n",
    "svc_precision = model_results[svc_model_name][\"precision\"]\n",
    "svc_recall = model_results[svc_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(svc_model_name)\n",
    "plot_confusion_matrix(svc_model_name, y_test, svc_y_test_predictions)\n",
    "plot_roc_curve(svc_model_name, y_test, svc_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(svc_model_name, y_test, svc_y_test_prediction_probabilities)\n",
    "plot_learning_curve(svc_best_model, X_train, y_train, svc_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_model_name = \"Naive Bayes\"\n",
    "\n",
    "naive_bayes_best_model = model_results[naive_bayes_model_name][\"best_model\"]\n",
    "\n",
    "naive_bayes_y_test_predictions = model_results[naive_bayes_model_name][\"y_test_predictions\"]\n",
    "naive_bayes_y_test_prediction_probabilities = model_results[naive_bayes_model_name][\"y_test_prediction_probabilities\"]\n",
    "\n",
    "naive_bayes_f1_score = model_results[naive_bayes_model_name][\"f1\"]\n",
    "naive_bayes_accuracy = model_results[naive_bayes_model_name][\"accuracy\"]\n",
    "naive_bayes_precision = model_results[naive_bayes_model_name][\"precision\"]\n",
    "naive_bayes_recall = model_results[naive_bayes_model_name][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation_results(naive_bayes_model_name)\n",
    "plot_confusion_matrix(naive_bayes_model_name, y_test, naive_bayes_y_test_predictions)\n",
    "plot_roc_curve(naive_bayes_model_name, y_test, naive_bayes_y_test_prediction_probabilities)\n",
    "plot_multiclass_precision_recall_curve(naive_bayes_model_name, y_test, naive_bayes_y_test_prediction_probabilities)\n",
    "plot_learning_curve(naive_bayes_best_model, X_train, y_train, naive_bayes_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-comparison-temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
